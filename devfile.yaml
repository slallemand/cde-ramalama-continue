schemaVersion: 2.2.0
metadata:
  generateName: cde-ramalama-continue
attributes:
  controller.devfile.io/storage-type: ephemeral
projects:
  - name: cde-ramalama-continue
    git:
      remotes:
        origin: 'https://github.com/redhat-developer-demos/cde-ramalama-continue'
      checkoutFrom:
        revision: master
components:
  - name: udi
    container:
      image: quay.io/devfile/universal-developer-image:ubi8-latest
      memoryLimit: 4Gi
      memoryRequest: 2Gi
      cpuLimit: 4000m
      cpuRequest: 1000m
      mountSources: true
      sourceMapping: /projects
  - name: ramalama
    attributes:
      container-overrides:
        resources:
          limits:
            cpu: 4000m
            memory: 12Gi
            # nvidia.com/gpu: 1 # Uncomment this if the pod shall be scheduled only on a GPU node
          requests:
            cpu: 1000m
            memory: 8Gi
            # nvidia.com/gpu: 1 # Uncomment this if the pod shall be scheduled only on a GPU node
    container:
      image: quay.io/ramalama/ramalama:0.15
      args:
        - "ramalama"
        - "--store"
        - "/models"
        - "serve"
        - "--network=none"
        - "ollama://codegemma:2b"
      volumeMounts:
        - name: ramalama-models
          path: /models
      endpoints:
        - exposure: public
          name: ramalamaserve
          protocol: http
          targetPort: 8080
  - name: ramalama-models
    volume:
      size: 5Gi
commands:
  - id: copyconfig
    exec:
      component: udi
      commandLine: "mkdir /home/user/.continue && cp /projects/cde-ramalama-continue/continue-config.yaml /home/user/.continue/config.yaml"
events:
  postStart:
    - copyconfig
